{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smv-manovihar/MechanicAI/blob/main/backend/JanathaGarage_Server.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output"
      ],
      "metadata": {
        "id": "OaffPlQzIGSh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installing necessary modules"
      ],
      "metadata": {
        "id": "L62zQurrIJtK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2sF591dADMLy"
      },
      "outputs": [],
      "source": [
        "!sudo apt-get install -y pciutils\n",
        "!curl -fsSL https://ollama.com/install.sh | sh #installing Ollama\n",
        "!pip install pyngrok flask-ngrok flask-cors pymongo sentence_transformers\n",
        "!ngrok authtoken 2jAeEXz0n15BRb4Vmjzh1OA1aw4_2EMyvpRPZ4TQymWyPhgKB\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating an Ollama instance in the environment"
      ],
      "metadata": {
        "id": "_YGZQ27fIX3U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3psvlX5-GP0_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import threading\n",
        "import subprocess\n",
        "import requests\n",
        "import json\n",
        "def ollama():\n",
        "    os.environ['OLLAMA_HOST'] = '0.0.0.0:11434'\n",
        "    os.environ['OLLAMA_ORIGINS'] = '*'\n",
        "    subprocess.Popen([\"ollama\", \"serve\"])\n",
        "\n",
        "ollama_thread = threading.Thread(target=ollama)\n",
        "ollama_thread.start()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading Llama 3.1 8B"
      ],
      "metadata": {
        "id": "CbpOWkiWIegn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yfRfHvsSqNf7"
      },
      "outputs": [],
      "source": [
        "!ollama pull llama3.1:8b\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installing LightRAG pytorch library"
      ],
      "metadata": {
        "id": "Rs6Gm2ZkIpI7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FjFr470eDRWb"
      },
      "outputs": [],
      "source": [
        "!pip install -U lightrag[ollama]\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ozc7mJ6DDRaL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from IPython.display import clear_output\n",
        "import threading\n",
        "from flask import Flask, request, jsonify\n",
        "from flask_ngrok import run_with_ngrok\n",
        "from flask_cors import CORS\n",
        "from pyngrok import ngrok\n",
        "from lightrag.core.generator import Generator\n",
        "from lightrag.core.component import Component\n",
        "from lightrag.core.model_client import ModelClient\n",
        "from lightrag.components.model_client import OllamaClient\n",
        "import time\n",
        "from pymongo import MongoClient\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import torch\n",
        "from google.colab import userdata\n",
        "from lightrag.components.model_client import OllamaClient\n",
        "from IPython.display import Markdown, display"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configuring Llama"
      ],
      "metadata": {
        "id": "C12UhFNoJNyx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "41IRQpg7YPPm"
      },
      "outputs": [],
      "source": [
        "qa_template = r\"\"\"<SYS>You are Repair Assistant who is an automotive expert. Your job is to assist users with car repairs, vehicle maintenance. Provide detailed and relevant information while following the structured response format below.\n",
        "\n",
        "Response Guidelines:\n",
        "\n",
        "Context Usage: Use the provided context to inform your response, USER wont be having access for this context DO NOT Recommend USER to refer the sections in the context, use it to impprove your response quality.\n",
        "\n",
        "Follow up requests: Do not encourage follow up for the prompts that are not related to car repairs. You can encourage follow up prompts when the user is on repair-related only.\n",
        "\n",
        "Scope: Only respond to queries related to car repairs, vehicle maintenance. Politely refuse if the query is unrelated or unclear. Do not answer about trips or food related or any other unrelated topics.\n",
        "\n",
        "Car parts explanation: Don't hesitate to explain if user asks what a certain part is. The context might not be useful in this case.\n",
        "\n",
        "Response Structure:\n",
        "\n",
        "  Diagnosis: Briefly explain the repair or the possible cause of the problem.\n",
        "  Instructions: Provide a step-by-step and detailed guide.\n",
        "  Tools Required: List all necessary tools.\n",
        "  Parts Replacement: Specify any parts that need replacement.\n",
        "  Safety Tips: Include key safety precautions.\n",
        "  Invalid Queries: Politely refuse to answer if the query is not relevant. your response for these type of queries should be maximum one sentence.\n",
        "\n",
        "</SYS>\n",
        "Context:\n",
        "      {{context}}\n",
        "\n",
        "#### User: {{input_str}}\n",
        "You:\"\"\"\n",
        "\n",
        "\n",
        "class LlamaGen:\n",
        "    def __init__(self, model_client, model_kwargs):\n",
        "        self.generator = Generator(model_client=model_client, model_kwargs=model_kwargs, template=qa_template)\n",
        "\n",
        "    def call(self, input, context):\n",
        "        response = self.generator.call({\"input_str\": str(input), \"context\": str(context)})\n",
        "        return response.data if hasattr(response, 'data') else str(response)  # Ensure response is a string\n",
        "\n",
        "    async def acall(self, input: dict, context: str) -> str:\n",
        "        return await self.generator.acall({\"input_str\": str(input), \"context\": str(context)})\n",
        "\n",
        "title_temp=r\"\"\"<SYS>\n",
        "      You are specialized in creating short summarized titles for a given text. Do not use quotes for the title\n",
        "      Give only the title as the output as below:\n",
        "      {generated title}\n",
        "</SYS>\n",
        "    Create a shortest title possible for the following text:\n",
        "    {{input_str}}\n",
        "You:\"\"\"\n",
        "\n",
        "class TitleGenertor:\n",
        "    def __init__(self, model_client, model_kwargs):\n",
        "        self.generator = Generator(model_client=model_client, model_kwargs=model_kwargs, template=title_temp)\n",
        "\n",
        "    def call(self, input):\n",
        "        response = self.generator.call({\"input_str\": str(input)})\n",
        "        return response.data if hasattr(response, 'data') else str(response)  # Ensure response is a string\n",
        "\n",
        "    async def acall(self, input: dict) -> str:\n",
        "        return await self.generator.acall({\"input_str\": str(input)})\n",
        "\n",
        "llm_model = {\n",
        "    \"model_client\": OllamaClient(),\n",
        "    \"model_kwargs\": {\"model\": \"llama3.1:8b\"},\n",
        "}\n",
        "qa = LlamaGen(**llm_model)\n",
        "title_gen = TitleGenertor(**llm_model);\n",
        "\n",
        "# prompt = \"my car is not starting\"\n",
        "# context = get_context(prompt)\n",
        "# print(context)\n",
        "# output = qa.call(prompt, context)\n",
        "# display(Markdown(f\"**Answer:**\"))\n",
        "# display(Markdown(f\"{output.data}\"))\n",
        "\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG"
      ],
      "metadata": {
        "id": "ftuuDqDyJWcZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client = MongoClient(\"mongodb+srv://DataPuller:janathagarage_read@janathagarage.sxw1j.mongodb.net/\")\n",
        "db = client['MechanicAI']\n",
        "collection = db['Hatchback']\n",
        "\n",
        "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
        "\n",
        "def semantic_search(query, top_k=5):\n",
        "    \"\"\"\n",
        "    Perform semantic search on MongoDB-stored embeddings.\n",
        "\n",
        "    Parameters:\n",
        "    - query (str): The search query.\n",
        "    - top_k (int): Number of top similar chunks to retrieve.\n",
        "\n",
        "    Returns:\n",
        "    - List of dictionaries containing 'score' and 'text' of top K chunks.\n",
        "    \"\"\"\n",
        "    # Step 1: Encode the query\n",
        "    query_embedding = model.encode(query, convert_to_tensor=True)\n",
        "\n",
        "    # Step 2: Fetch all documents with embeddings\n",
        "    cursor = collection.find({}, {'text': 1, 'embeddings': 1})  # Adjust fields if necessary\n",
        "\n",
        "    # Prepare lists to store texts and embeddings\n",
        "    texts = []\n",
        "    embeddings = []\n",
        "\n",
        "    for doc in cursor:\n",
        "        texts.append(doc['text'])\n",
        "        embeddings.append(doc['embeddings'])\n",
        "\n",
        "    # Convert list of embeddings to a tensor and move to the same device as query_embedding\n",
        "    corpus_embeddings = torch.tensor(embeddings).to(query_embedding.device) # Move to the device of query_embedding\n",
        "\n",
        "    # Step 3: Compute cosine similarity between query and all embeddings\n",
        "    cos_scores = util.cos_sim(query_embedding, corpus_embeddings)[0]\n",
        "\n",
        "    # Step 4: Retrieve the top K chunks with highest similarity scores\n",
        "    top_results = torch.topk(cos_scores, k=top_k)\n",
        "\n",
        "    # Collect the top K results\n",
        "    top_chunks = []\n",
        "    for score, idx in zip(top_results.values, top_results.indices):\n",
        "        top_chunks.append({\n",
        "            'score': score.item(),\n",
        "            'text': texts[idx]\n",
        "        })\n",
        "\n",
        "    return top_chunks\n",
        "\n",
        "def get_context(query):\n",
        "    # query_embedding = get_query_embedding(query)  # Get the embedding for the query\n",
        "    top_k_sections = semantic_search(query, top_k=2)\n",
        "    if  top_k_sections[0][\"score\"] < 0.3:\n",
        "      return \"\"\n",
        "    context = \" \".join([section[\"text\"] for section in top_k_sections])\n",
        "    return context"
      ],
      "metadata": {
        "id": "f4H6VL9-JVPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Flask App"
      ],
      "metadata": {
        "id": "RBnsqBN3Kp0Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7JuOcSHSDRdJ",
        "outputId": "18c19514-66c5-49cc-fa93-a184ed6a2d99"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug:127.0.0.1 - - [24/Oct/2024 10:14:55] \"POST /chat HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [24/Oct/2024 10:15:47] \"POST /chat HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [24/Oct/2024 10:17:52] \"POST /chat HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [24/Oct/2024 10:18:27] \"POST /chat HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [24/Oct/2024 10:22:47] \"POST /chat HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [24/Oct/2024 10:26:16] \"POST /chat HTTP/1.1\" 200 -\n"
          ]
        }
      ],
      "source": [
        "# Setup Flask app\n",
        "app = Flask(__name__)\n",
        "CORS(app)\n",
        "#run_with_ngrok(app)\n",
        "@app.route('/chat', methods=['POST'])\n",
        "def chat():\n",
        "    data = request.json\n",
        "    user_prompt = data['prompt']\n",
        "    new = data['new']\n",
        "    context = get_context(user_prompt)\n",
        "    output = qa.call(user_prompt, context)\n",
        "    if new:\n",
        "      title = title_gen.call(\"User:\\n\"+user_prompt+\"Assitant:\\n\"+output)\n",
        "      return jsonify({'response': output, 'title': title})\n",
        "    # Remove Markdown asterisks\n",
        "    clean_output = output.replace(\"**\", \"\").replace(\"*\", \"\")\n",
        "    return jsonify({'response': clean_output})\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Set up ngrok with custom domain\n",
        "    get_ipython().system_raw('ngrok http --domain=koi-wanted-mayfly.ngrok-free.app 5000 &')\n",
        "    app.run(port=5000)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}